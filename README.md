# Code Completion Analysis

An analysis of code completion suggestions generated by starcoderbase-1b on Python source code. 

The entire code can be found in the `src/main.ipynb` Jupyter Notebook. Source code examples are taken from `data` directory. Results are produced in the `out` directory. CSV files with results of the execution and metrics can be found in the `out/tables` directory.

## Methodology

The entire process can be summarized in five steps:

1. Read all files from the `data` directory into strings.
2. Remove a part of the code from each file.
3. Ask the language model (LM) to generate a suggestion of what might be missing, given the context.
4. Output the results.
5. Calculate metrics.

### Input
Six Python source code snippets were taken as the input. They are all under 25 lines of codes long. Some code examples are simpler than others.

### Splitting the Code
Each loaded file is split into three consecutive parts: prefix, middle, and suffix. The middle part is what gets removed, while prefix and suffix is what is being fed to the LM. The splits are loaded into `split_data` matrix of dimensions `[n_files x n_samples x 3]`.

A parameter `n_samples` controls how many such splits are done for a single file. Parameters `max_missing_chars` and `min_missing_chars` control the size of the middle part. The size is chosen randomly in that range, so is the starting index of the middle part. The randomizer is seeded with a number so more reproducible results can be achieved. That number was picked randomly before being hardcoded.

### Querying the Model
The prefix and suffix are used as the input to `bigcode/starcoderbase-1b`. That model supports fill-in-the-middle (FIM) queries which makes it a good choice for this task since it doesn't require engineering a prompt.

### Output
Modified source code files are saved in the `out` directory. A CSV file with original text and the text suggested by the model are saved in the `out/tables/results.csv`.

### Metrics
I calculated five metrics for the acquired results: 

- fraction of exact matches
- chrF
- BLEU
- SentenceBERT
- fraction of the files that are valid syntactically

The summarized metrics can be found under `out/tables/metrics.csv` and the individual ones can be found under `out/tables/metrics_individual.csv`.

## Manual Labeling
I manually labeled all the obtained results in order to assess them. There are six categories:

1. Exact match - suggestions that match perfectly, character by character.
2. Equivalent match - suggestions that do not match perfectly, but the code compiles AND produces the same outputs AND there is no redundant code.
3. Close match - suggestions that do not match perfectly, but the code compiles with possibly different outputs due to lack of information (context). Some redundant code is allowed.
4. Syntactic match - suggestions that compile, but the results might be different due to logical errors, e.g. edge cases are not accounted for. Redundant code is allowed.
5. Semantic match - suggestions that do not compile, but the idea is close to the original idea (with possible redundant code).
6. Total miss - suggestions that do not fit any of the categories above.

The labeled results can be found in the `out/tables/results_labled.csv` file.
