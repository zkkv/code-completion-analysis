{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: read all files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 file(s) have been read in directory ../data\n"
     ]
    }
   ],
   "source": [
    "def read_files(directory_path: str, filenames: list[str]) -> list[str]:\n",
    "\tfile_contents = []\n",
    "\n",
    "\tfor filename in filenames:\n",
    "\t\tfile_path = os.path.join(directory_path, filename)\n",
    "\n",
    "\t\twith open(file_path, 'r') as file:\n",
    "\t\t\tfile_contents.append(file.read())\n",
    "\n",
    "\treturn file_contents\n",
    "\n",
    "\n",
    "in_directory_path = \"../data\"\n",
    "filenames = os.listdir(in_directory_path)\n",
    "contents = read_files(in_directory_path, filenames)\n",
    "n_files = len(contents)\n",
    "\n",
    "print(f\"{n_files} file(s) have been read in directory {in_directory_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: split each file into three parts multiple times\n",
    "\n",
    "`split_data` will be of dimensions `[n_files x n_samples x 3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_data has been initialized with 1 sample(s) per file\n"
     ]
    }
   ],
   "source": [
    "def split(n_samples: int, min_missing_chars: int, max_missing_chars: int) -> list[list[list[str]]]:\n",
    "\tsplit_data = []\n",
    "\n",
    "\tfor file_content in contents:\n",
    "\t\tfile_splits = []\n",
    "\t\tfor _ in range(n_samples):\n",
    "\t\t\tmiddle_len = min(len(file_content), random.randint(min_missing_chars, max_missing_chars))\n",
    "\t\t\tmiddle_start = random.randint(0, len(file_content) - 1 - middle_len)\n",
    "\t\t\tmiddle_end = middle_start + middle_len\n",
    "\n",
    "\t\t\tprefix = file_content[:middle_start]\n",
    "\t\t\tmiddle = file_content[middle_start:middle_end]\n",
    "\t\t\tsuffix = file_content[middle_end:]\n",
    "\n",
    "\t\t\tfile_splits.append([prefix, middle, suffix])\n",
    "\t\tsplit_data.append(file_splits)\n",
    "\treturn split_data\n",
    "\n",
    "\n",
    "n_samples = 1  # Number of different splits obtained per file. Total number of samples = n_files * n_samples\n",
    "max_missing_chars = 50  # The maximum number of characters that need to be completed (i.e. the middle part)\n",
    "min_missing_chars = 5\n",
    "random.seed(45)  # For reproducible results\n",
    "\n",
    "split_data = split(n_samples, min_missing_chars, max_missing_chars)\n",
    "\n",
    "print(f\"split_data has been initialized with {n_samples} sample(s) per file\")\n",
    "\n",
    "# Uncomment to see an example\n",
    "# print(\"Prefix:\", split_data[0][0][0])\n",
    "# print(\"Middle:\", split_data[0][0][1])\n",
    "# print(\"Suffix:\", split_data[0][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1: Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_login() -> None:\n",
    "\tload_dotenv()\n",
    "\tapi_key = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\tlogin(api_key)\n",
    "\n",
    "\n",
    "def load_model(model_name: str, device: str):\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\ttokenizer.pad_token = tokenizer.eos_token\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16).to(device)\n",
    "\treturn model, tokenizer\n",
    "\n",
    "\n",
    "hf_login()\n",
    "device = \"cpu\"\n",
    "# model_name = \"bigcode/tiny_starcoder_py\"\n",
    "model_name = \"bigcode/starcoderbase-1b\"\n",
    "model, tokenizer = load_model(model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2: Use the model to generate a suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest(model, tokenizer, prefix: str, suffix: str) -> str:\n",
    "\tPREFIX_TAG = \"<fim_prefix>\"\n",
    "\tSUFFIX_TAG = \"<fim_suffix>\"\n",
    "\tMIDDLE_TAG = \"<fim_middle>\"\n",
    "\tEOT_TAG = \"<|endoftext|>\"\n",
    "\t\n",
    "\tinput_text = f\"{PREFIX_TAG}{prefix}{SUFFIX_TAG}{suffix}{MIDDLE_TAG}\"\n",
    "\tinputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\toutputs = model.generate(inputs, max_new_tokens=max_missing_chars, pad_token_id=tokenizer.pad_token_id)\n",
    "\tresult = tokenizer.decode(outputs[0])\n",
    "\treturn result.split(MIDDLE_TAG)[-1].replace(EOT_TAG, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMOVE ME\n",
    "# test_prefix = \"# Return sum of two numbers\\ndef add(a, b):\\n    \"\n",
    "# test_suffix = \"+ b\"\n",
    "# test_result = suggest(model, tokenizer, test_prefix, test_suffix)\n",
    "# print(test_result)\n",
    "\n",
    "# # test_prefix = \"def print_one_two_three():\\n    print('one')\\n    \"\n",
    "# # test_suffix = \"\\n    print('three')\"\n",
    "# # test_result = suggest(test_prefix, test_suffix)\n",
    "# # print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_suggestions = []\n",
    "\n",
    "for i in range(n_files):\n",
    "\tfile_suggestions = []\n",
    "\tfor j in range(n_samples):\n",
    "\t\tsuggestion = suggest(model, tokenizer, split_data[i][j][0], split_data[i][j][2])\n",
    "\t\tfile_suggestions.append(suggestion)\n",
    "\traw_suggestions.append(file_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Original</th>\n",
       "      <th>Suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>animal.py</td>\n",
       "      <td>f):\\n\\t\\tself.weight_kg =</td>\n",
       "      <td>f):\\n\\t\\tself.weight_kg = 10\\n\\t\\tself.sound =...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add.py</td>\n",
       "      <td>sum of two numbers\\ndef add(a, b):\\n\\t</td>\n",
       "      <td>the sum of two numbers\\ndef add(a, b):\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File                                 Original  \\\n",
       "0  animal.py                f):\\n\\t\\tself.weight_kg =   \n",
       "1     add.py   sum of two numbers\\ndef add(a, b):\\n\\t   \n",
       "\n",
       "                                          Suggestion  \n",
       "0  f):\\n\\t\\tself.weight_kg = 10\\n\\t\\tself.sound =...  \n",
       "1       the sum of two numbers\\ndef add(a, b):\\n      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_table() -> pd.DataFrame:\n",
    "\tdata = []\n",
    "\tfor i in range(n_files):\n",
    "\t\tfile_name = filenames[i]\n",
    "\t\tfor j in range(n_samples):\n",
    "\t\t\trow = {\n",
    "\t\t\t\t\"File\": file_name,\n",
    "\t\t\t\t\"Original\": split_data[i][j][1],\n",
    "\t\t\t\t\"Suggestion\": raw_suggestions[i][j]\n",
    "\t\t\t}\n",
    "\t\t\tdata.append(row)\n",
    "\n",
    "\treturn pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def write_table(table_directory_path: str, table: pd.DataFrame) -> None:\n",
    "\tfile_path = \"results.csv\"\n",
    "\tif table_directory_path[-1] != \"/\":\n",
    "\t\ttable_directory_path += \"/\"\n",
    "\tfull_path = f\"{table_directory_path}{file_path}\"\n",
    "\ttable.to_csv(full_path, index=False)\n",
    "\n",
    "\n",
    "table_directory_path = \"../out/tables\"\n",
    "table = get_table()\n",
    "display(table)\n",
    "write_table(table_directory_path, table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_number_to_filename(filename: str, number: int) -> str:\n",
    "\tname, extension = filename.split(\".\")\n",
    "\treturn f\"{name}_{number}.{extension}\"\n",
    "\n",
    "\n",
    "def reconstruct_data(prefix: str, middle: str, suffix: str) -> str:\n",
    "\treturn prefix + middle + suffix\n",
    "\n",
    "\n",
    "def write_file(directory_path: str, filename: str, data: str) -> None:\n",
    "\tfile_path = os.path.join(directory_path, filename)\n",
    "\n",
    "\twith open(file_path, 'w') as file:\n",
    "\t\tfile.write(data)\n",
    "\n",
    "\n",
    "def write_files(directory_path: str) -> None:\n",
    "\tfor i in range(n_files):\n",
    "\t\tfor j in range(n_samples):\n",
    "\t\t\tfilename = append_number_to_filename(filenames[i], j)\n",
    "\t\t\tdata = reconstruct_data(split_data[i][j][0], raw_suggestions[i][j], split_data[i][j][2])\n",
    "\t\t\twrite_file(directory_path, filename, data)\n",
    "\n",
    "\n",
    "out_directory_path = \"../out\"\n",
    "write_files(out_directory_path)\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def suggest(prefix: str, suffix: str) -> str:\n",
    "# \tcheckpoint = \"bigcode/tiny_starcoder_py\"\n",
    "# \tdevice = \"cpu\"\n",
    "\n",
    "# \ttokenizer = MistralTokenizer.v3()\n",
    "# \tmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# \tprefix = \"\"\"def add(\"\"\"\n",
    "# \tsuffix = \"\"\"    return sum\"\"\"\n",
    "\n",
    "# \trequest = FIMRequest(prompt=prefix, suffix=suffix)\n",
    "\n",
    "# \ttokens = tokenizer.encode_fim(request).tokens\n",
    "\n",
    "# \tout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "# \tresult = tokenizer.decode(out_tokens[0])\n",
    "\n",
    "# \tmiddle = result.split(suffix)[0].strip()\n",
    "# \tprint(middle)\n",
    "# \treturn middle\n",
    "\n",
    "\n",
    "# hf_login()\n",
    "# raw_suggestions = []\n",
    "\n",
    "# for i in range(n_files):\n",
    "# \tfile_suggestions = []\n",
    "# \tfor j in range(n_samples):\n",
    "# \t\tsuggestion = suggest(split_data[i][j][0], split_data[i][j][2])\n",
    "# \t\tfile_suggestions.append(suggestion)\n",
    "# \traw_suggestions.append(file_suggestions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def suggest(prefix: str, suffix: str) -> str:\n",
    "# \tfrom transformers import GemmaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# \tmodel_id = \"google/codegemma-7b\"\n",
    "# \ttokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
    "# \tmodel = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# \tprompt = '''\\\n",
    "# \t<|fim_prefix|>import datetime\n",
    "# \tdef calculate_age(birth_year):\n",
    "# \t\t\"\"\"Calculates a person's age based on their birth year.\"\"\"\n",
    "# \t\tcurrent_year = datetime.date.today().year\n",
    "# \t\t<|fim_suffix|>\n",
    "# \t\treturn age<|fim_middle|>\\\n",
    "# \t'''\n",
    "\n",
    "# \tinputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# \tprompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "# \toutputs = model.generate(**inputs, max_new_tokens=100)\n",
    "# \tprint(tokenizer.decode(outputs[0][prompt_len:]))\n",
    "\n",
    "\n",
    "# # hf_login()\n",
    "# raw_suggestions = []\n",
    "\n",
    "# for i in range(n_files):\n",
    "# \tfile_suggestions = []\n",
    "# \tfor j in range(n_samples):\n",
    "# \t\tsuggestion = suggest(split_data[i][j][0], split_data[i][j][2])\n",
    "# \t\tfile_suggestions.append(suggestion)\n",
    "# \traw_suggestions.append(file_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Replace with your Hugging Face API token\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('HUGGINGFACE_TOKEN')\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\"\n",
    "# }\n",
    "\n",
    "# prompt = '''\\\n",
    "# <|fim_prefix|>import datetime\n",
    "# def calculate_age(birth_year):\n",
    "# \t\"\"\"Calculates a person's age based on their birth year.\"\"\"\n",
    "# \tcurrent_year = datetime.date.today().year\n",
    "# \t<|fim_suffix|>\n",
    "# \treturn age<|fim_middle|>\\\n",
    "# '''\n",
    "\n",
    "# # Sample input for the model\n",
    "# data = {\n",
    "#     \"inputs\": {\n",
    "#         \"text\": prompt\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Send the request to the API\n",
    "# response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "# # Get the output\n",
    "# result = json.loads(response.content.decode(\"utf-8\"))\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Transformer.from_folder(\"~/codestral-22B-240529\")\n",
    "\n",
    "# prefix = \"\"\"def add(\"\"\"\n",
    "# suffix = \"\"\"    return sum\"\"\"\n",
    "\n",
    "# request = FIMRequest(prompt=prefix, suffix=suffix)\n",
    "\n",
    "# tokens = tokenizer.encode_fim(request).tokens\n",
    "\n",
    "# out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "# result = tokenizer.decode(out_tokens[0])\n",
    "\n",
    "# middle = result.split(suffix)[0].strip()\n",
    "# print(middle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# hf_login()\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "# outputs = pipe(\n",
    "#     messages,\n",
    "#     max_new_tokens=256,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "3\n",
      "========0, 0========\n",
      "=======PREFIX=======\n",
      " class Dog:\n",
      "\tdef __init__(self):\n",
      "\t\tself.weight_kg = 10\n",
      "\t\tself.sound = \"Bark\"\n",
      "\n",
      "\n",
      "class Cat:\n",
      "\tdef __init__(sel\n",
      "=======MIDDLE=======\n",
      " f):\n",
      "\t\tself.weight_kg =\n",
      "=======SUFFIX=======\n",
      "  4\n",
      "\t\tself.sound = \"Meow\"\n",
      "=======RAW=======\n",
      " f):\n",
      "\t\tself.weight_kg = 10\n",
      "\t\tself.sound = \"Meow\"\n",
      "\n",
      "\n",
      "class DogCat:\n",
      "\tdef __init__(self):\n",
      "\t\tself.weight_kg =\n",
      "========1, 0========\n",
      "=======PREFIX=======\n",
      " # Return\n",
      "=======MIDDLE=======\n",
      "  sum of two numbers\n",
      "def add(a, b):\n",
      "\t\n",
      "=======SUFFIX=======\n",
      " return a + b\n",
      "\n",
      "=======RAW=======\n",
      "  the sum of two numbers\n",
      "def add(a, b):\n",
      "    \n",
      "========2, 0========\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m to_print:\n\u001b[1;32m      7\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m========\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m========\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======PREFIX=======\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[43msplit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[j][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======MIDDLE=======\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,split_data[i][j][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     10\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======SUFFIX=======\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,split_data[i][j][\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "to_print = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0)]\n",
    "print(len(split_data))\n",
    "print(len(split_data[0]))\n",
    "print(len(split_data[0][0]))\n",
    "\n",
    "for i, j in to_print:\n",
    "\tprint(f'========{i}, {j}========')\n",
    "\tprint(\"=======PREFIX=======\\n\",split_data[i][j][0])\n",
    "\tprint(\"=======MIDDLE=======\\n\",split_data[i][j][1])\n",
    "\tprint(\"=======SUFFIX=======\\n\",split_data[i][j][2])\n",
    "\tprint(\"=======RAW=======\\n\",raw_suggestions[i][j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
